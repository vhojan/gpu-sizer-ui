// src/components/SessionTokenParams.jsx
import React from "react";

export default function SessionTokenParams({
  users, setUsers,
  latency, setLatency,
  sessionTokens, setSessionTokens,
  modelDetails,
  TOKEN_SIZE
}) {
  // Defensive: compute minLatency safely!
  const minLatency = (
    modelDetails &&
    typeof modelDetails["Base Latency (s)"] === "number" &&
    !isNaN(modelDetails["Base Latency (s)"])
  ) ? Math.round(modelDetails["Base Latency (s)"] * 1000) : 1;

  return (
    <div>
      <h2 className="text-xl font-semibold mb-2">Session/Token Parameters</h2>
      <div className="bg-gray-100 dark:bg-gray-800 p-4 rounded grid grid-cols-1 md:grid-cols-2 gap-6">
        {/* Concurrent Sessions */}
        <label>
          <span className="font-medium">Concurrent Sessions</span>
          <input
            type="number"
            min="1"
            disabled={!modelDetails}
            className="mt-1 block w-full p-2 border rounded bg-white dark:bg-gray-800"
            value={users}
            onChange={e => setUsers(Number(e.target.value))}
          />
          <div className="text-xs mt-1 text-gray-500 dark:text-gray-400">
            Adding more concurrent sessions adds more KVCache reservations and adds up the average number of tokens/seconds for a session. Adding more users automatically means you are asking more computational and memory resources from the GPU.
          </div>
        </label>
        {/* First Time to Token Latency */}
        <label>
          <span className="font-medium">First Time to Token Latency (ms)</span>
          <div className="flex items-center space-x-2 mt-1">
            <span className="font-mono text-sm">{latency} ms</span>
          </div>
          <input
            type="range"
            min={minLatency}
            max={30000}
            step={50}
            value={latency}
            disabled={!modelDetails}
            onChange={e => setLatency(Number(e.target.value))}
            className="w-full mt-2"
          />
          <div className="text-xs mt-1 text-gray-500 dark:text-gray-400">
            The initial latency is the minimal achievable latency for a single session. Increasing users without increasing the first time to token latency will require more GPU cores and could ask for multiple GPUs if it exceeds a single one.
          </div>
        </label>
        {/* Tokens per Second per Session */}
        <label>
          <span className="font-medium">Average Context Length</span>
          <div className="flex items-center space-x-2 mt-1">
            <span className="font-mono text-sm">{sessionTokens} tokens</span>
          </div>
          <input
            type="range"
            min={1}
            max={6000}
            step={1}
            disabled={!modelDetails}
            value={sessionTokens}
            onChange={e => setSessionTokens(Number(e.target.value))}
            className="w-full mt-2"
          />
          <div className="text-xs mt-1 text-gray-500 dark:text-gray-400">
            This controls the average tokens per second generated by each concurrent session.
          </div>
        </label>
        {/* Token Size */}
        <label>
          <span className="font-medium">Token Size</span>
          <input
            className="mt-1 block w-full p-2 border rounded bg-white dark:bg-gray-800"
            value={`${TOKEN_SIZE} bytes`}
            readOnly
          />
          <div className="text-xs mt-1 text-gray-500 dark:text-gray-400">
            Token size = memory size per token (e.g., 2 bytes for FP16).
            Typical LLMs use 2 bytes/token, but this can vary.<br />
            A token usually represents ~4 characters of text.
          </div>
        </label>
      </div>
    </div>
  );
}